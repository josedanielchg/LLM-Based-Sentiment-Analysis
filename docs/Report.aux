\relax 
\citation{b1}
\citation{b2}
\citation{b3}
\citation{b4}
\citation{b5}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Q0-Dataset analysis with different classical machine learning model}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Exploratory data analysis (EDA)}{1}{}\protected@file@percent }
\citation{b5}
\citation{b5}
\citation{b6}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Distribution of sentiments in training dataset.\relax }}{2}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:q0-eda}{{1}{2}}
\newlabel{eq:tfidf}{{1}{2}}
\newlabel{eq:idf}{{2}{2}}
\newlabel{eq:l2norm}{{3}{2}}
\newlabel{fig:q0-bow-uni}{{2a}{2}}
\newlabel{sub@fig:q0-bow-uni}{{(a)}{a}}
\newlabel{fig:q0-bow-bi}{{2b}{2}}
\newlabel{sub@fig:q0-bow-bi}{{(b)}{b}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Top 20 n-gramas with BoW.\relax }}{2}{}\protected@file@percent }
\newlabel{fig:q0-bow}{{2}{2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Monograms}}}{2}{}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Bigrams}}}{2}{}\protected@file@percent }
\citation{b7}
\newlabel{fig:q0-tfidf-uni}{{3a}{3}}
\newlabel{sub@fig:q0-tfidf-uni}{{(a)}{a}}
\newlabel{fig:q0-tfidf-bi}{{3b}{3}}
\newlabel{sub@fig:q0-tfidf-bi}{{(b)}{b}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Top 20 n-gramas with TF--IDF.\relax }}{3}{}\protected@file@percent }
\newlabel{fig:q0-tfidf}{{3}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Monograms}}}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Bigrams}}}{3}{}\protected@file@percent }
\newlabel{fig:q0-sbu}{{4a}{3}}
\newlabel{sub@fig:q0-sbu}{{(a)}{a}}
\newlabel{fig:q0-sbtt}{{4b}{3}}
\newlabel{sub@fig:q0-sbtt}{{(b)}{b}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparative sentiment distribution by category, stratified by user and tweet type\relax }}{3}{}\protected@file@percent }
\newlabel{fig:q0-sbuubtt}{{4}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Sentiments by User}}}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Sentiments by type of tweet}}}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Distribution of sentiments in the top 10 countries.\relax }}{3}{}\protected@file@percent }
\newlabel{fig:q0-dcountry}{{5}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}Classic model training}{3}{}\protected@file@percent }
\citation{b8}
\citation{b9}
\citation{b11}
\citation{b12}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Word count distribution by sentiment\relax }}{4}{}\protected@file@percent }
\newlabel{fig:q0-wordcount}{{6}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Q1-Classification with Classical Models and Performance Analysis}{5}{}\protected@file@percent }
\newlabel{sec:q1}{{III}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Training time}{5}{}\protected@file@percent }
\newlabel{subsec:q1-time}{{\mbox  {III-A}}{5}}
\newlabel{subsubsec:q1-ttotal}{{\mbox  {III-A}1}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-A}1}Total time estimate (sequential compute)}{5}{}\protected@file@percent }
\newlabel{eq:q1-e1}{{4}{5}}
\newlabel{subsubsec:q1-twall}{{\mbox  {III-A}2}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-A}2}Wall-clock time with parallelization}{5}{}\protected@file@percent }
\newlabel{eq:q1-e2}{{5}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Test-set performance}{5}{}\protected@file@percent }
\newlabel{subsec:q1-test}{{\mbox  {III-B}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Test comparison --- Macro-F1 (hue = vectorization).\relax }}{5}{}\protected@file@percent }
\newlabel{fig:q1-f1}{{7}{5}}
\newlabel{subsubsec:q1-vectorization}{{\mbox  {III-B}1}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-B}1}Impact of the vectorization method}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Test comparison --- Macro-Recall (hue = vectorization).\relax }}{6}{}\protected@file@percent }
\newlabel{fig:q1-f2}{{8}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Test comparison --- Accuracy (hue = vectorization).\relax }}{6}{}\protected@file@percent }
\newlabel{fig:q1-f3}{{9}{6}}
\newlabel{subsubsec:q1-tradeoffs}{{\mbox  {III-B}2}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-B}2}Model comparison and trade-offs}{6}{}\protected@file@percent }
\newlabel{subsubsec:q1-cm}{{\mbox  {III-B}3}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-B}3}Confusion matrix analysis (TF--IDF char)}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Confusion matrix of \texttt  {best\_\_tfidf\_char\_\_svm}.\relax }}{6}{}\protected@file@percent }
\newlabel{fig:q1-cm-svm}{{10}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Confusion matrix of \texttt  {best\_\_tfidf\_char\_\_lr}.\relax }}{6}{}\protected@file@percent }
\newlabel{fig:q1-cm-lr}{{11}{6}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {III-B}3a}General pattern: the ``neutral'' class is the main source of ambiguity.}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {III-B}3b}Good sign: low direct confusion between ``negative'' and ``positive''.}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {III-B}3c}Subtle differences between SVM and Logistic Regression.}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Top-20 and Bottom-20 character n-grams per class (LR vs.\ SVM)}{7}{}\protected@file@percent }
\newlabel{subsec:q1-ngrams}{{\mbox  {III-C}}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-C}1}Positive class}{7}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Positive class: Top-20 character n-grams (Logistic Regression) under TF--IDF char (\texttt  {char\_wb}).\relax }}{7}{}\protected@file@percent }
\newlabel{fig:q1-pos-lr-top}{{12}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Positive class: Top-20 character n-grams (Linear SVM) under TF--IDF char (\texttt  {char\_wb}).\relax }}{7}{}\protected@file@percent }
\newlabel{fig:q1-pos-svm-top}{{13}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-C}2}Negative class}{7}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Positive class: Bottom-20 character n-grams (Logistic Regression) under TF--IDF char (\texttt  {char\_wb}).\relax }}{8}{}\protected@file@percent }
\newlabel{fig:q1-pos-lr-bottom}{{14}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Positive class: Bottom-20 character n-grams (Linear SVM) under TF--IDF char (\texttt  {char\_wb}).\relax }}{8}{}\protected@file@percent }
\newlabel{fig:q1-pos-svm-bottom}{{15}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Negative class: Top-20 character n-grams (Logistic Regression) under TF--IDF char (\texttt  {char\_wb}).\relax }}{8}{}\protected@file@percent }
\newlabel{fig:q1-neg-lr-top}{{16}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Negative class: Top-20 character n-grams (Linear SVM) under TF--IDF char (\texttt  {char\_wb}).\relax }}{8}{}\protected@file@percent }
\newlabel{fig:q1-neg-svm-top}{{17}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Negative class: Bottom-20 character n-grams (Logistic Regression) under TF--IDF char (\texttt  {char\_wb}).\relax }}{8}{}\protected@file@percent }
\newlabel{fig:q1-neg-lr-bottom}{{18}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Negative class: Bottom-20 character n-grams (Linear SVM) under TF--IDF char (\texttt  {char\_wb}).\relax }}{8}{}\protected@file@percent }
\newlabel{fig:q1-neg-svm-bottom}{{19}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Neutral class: Top-20 character n-grams (Logistic Regression) under TF--IDF char (\texttt  {char\_wb}).\relax }}{9}{}\protected@file@percent }
\newlabel{fig:q1-neu-lr-top}{{20}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Neutral class: Top-20 character n-grams (Linear SVM) under TF--IDF char (\texttt  {char\_wb}).\relax }}{9}{}\protected@file@percent }
\newlabel{fig:q1-neu-svm-top}{{21}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-C}3}Neutral class}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-D}}Synthesis and practical considerations}{9}{}\protected@file@percent }
\newlabel{subsec:q1-synthesis}{{\mbox  {III-D}}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Neutral class: Bottom-20 character n-grams (Logistic Regression) under TF--IDF char (\texttt  {char\_wb}).\relax }}{9}{}\protected@file@percent }
\newlabel{fig:q1-neu-lr-bottom}{{22}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Neutral class: Bottom-20 character n-grams (Linear SVM) under TF--IDF char (\texttt  {char\_wb}).\relax }}{9}{}\protected@file@percent }
\newlabel{fig:q1-neu-svm-bottom}{{23}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Q2-MLP Architectures for Short-Text Sentiment Classification}{9}{}\protected@file@percent }
\citation{b13}
\citation{b14}
\citation{b15}
\citation{b16}
\citation{b17}
\citation{b18}
\@writefile{toc}{\contentsline {section}{\numberline {V}Q5-Bert embeddings vs. Raw representations}{12}{}\protected@file@percent }
\citation{b19}
\newlabel{eq:param_count_fc}{{6}{13}}
\newlabel{eq:param_count_mlp_char}{{7}{13}}
\newlabel{eq:param_count_mlp_bert}{{8}{13}}
\citation{b_bert}
\citation{b_att}
\citation{b_bert}
\citation{b_bert}
\citation{b_bert}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Q6-BERT Architecture and Theoretical Framework}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-A}}Model Architecture}{14}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Differences in pre-training model architectures. BERT uses a bidirectional Transformer, distinguishing it from OpenAI GPT and ELMo \cite  {b_bert}.\relax }}{14}{}\protected@file@percent }
\newlabel{fig:bert_arch}{{24}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-A}1}Multi-Head Self-Attention}{14}{}\protected@file@percent }
\newlabel{eq:attention}{{9}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-A}2}Feed-Forward Network and GELU Activation}{14}{}\protected@file@percent }
\newlabel{eq:ffn}{{10}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-A}3}Residual Connections and Normalization}{14}{}\protected@file@percent }
\newlabel{eq:addnorm}{{11}{14}}
\citation{b_bert}
\citation{b_bert}
\citation{b_bert}
\citation{b_bert}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-B}}Input Representation}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-B}1}WordPiece Tokenization}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-B}2}Embedding Summation}{15}{}\protected@file@percent }
\newlabel{eq:embeddings}{{12}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings, and the position embeddings \cite  {b_bert}.\relax }}{15}{}\protected@file@percent }
\newlabel{fig:bert_input}{{25}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-C}}Pre-training Objectives}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-C}1}Masked Language Model (MLM)}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-C}2}Next Sentence Prediction (NSP)}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-D}}Application Strategies}{15}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Feature-based approach results on CoNLL-2003 NER. Concatenating the last 4 hidden layers yields performance comparable to fine-tuning \cite  {b_bert}.\relax }}{15}{}\protected@file@percent }
\newlabel{fig:bert_table}{{26}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Conclusion}{15}{}\protected@file@percent }
\bibcite{b1}{1}
\bibcite{b2}{2}
\bibcite{b3}{3}
\bibcite{b4}{4}
\bibcite{b5}{5}
\bibcite{b6}{6}
\bibcite{b7}{7}
\bibcite{b8}{8}
\bibcite{b9}{9}
\bibcite{b10}{10}
\bibcite{b11}{11}
\bibcite{b12}{12}
\bibcite{b13}{13}
\bibcite{b14}{14}
\@writefile{toc}{\contentsline {section}{References}{16}{}\protected@file@percent }
\bibcite{b15}{15}
\bibcite{b16}{16}
\bibcite{b17}{17}
\bibcite{b18}{18}
\bibcite{b19}{19}
\bibcite{b_bert}{20}
\bibcite{b_att}{21}
\gdef \@abspage@last{17}
