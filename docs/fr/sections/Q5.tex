\section{Q5-Embeddings BERT vs. Représentations brutes}

Bien que la différence impliquée par la représentation du texte brut au moyen de l’une des méthodes classiques de vectorisation basées sur la fréquence et l’utilisation d’embeddings BERT ait déjà été abordée dans la section sur l’entraînement du MLP, il est pertinent de discuter plus en détail la différence entre ces deux formes de représentation numérique des éléments textuels : les approches basées sur des comptages d’occurrences en fréquence et les représentations basées sur les transformeurs.

Le premier élément qui ressort parmi ces différences est que BERT est un encodeur de transformeur pré-entraîné avec l’objectif de produire des représentations contextuelles bidirectionnelles. Cela signifie que les représentations de tokens résultantes sont encodées en utilisant à la fois le contexte gauche et le contexte droit, c’est-à-dire les mots qui apparaissent avant et après chaque token dans la séquence tokenisée \cite{b18}.

BERT produit des représentations contextualisées au niveau des tokens qui, via une opération de pooling, donnent un embedding dense de taille fixe par texte. Cet embedding correspond à un vecteur de dimension 768, qui devient la dimension d’entrée du classifieur. Cette différence significative de dimensionnalité entre l’utilisation d’un modèle de vectorisation basé sur la fréquence et l’utilisation d’embeddings BERT est le premier facteur différenciant qui motive l’utilisation de ces derniers plutôt que l’approche traditionnelle.

Pour mettre en évidence l’effet que ce changement de dimensionnalité a sur l’entraînement du modèle classifieur, il est proposé d’isoler le MLP du pipeline et d’analyser le nombre de paramètres du réseau en fonction de son entrée, et de comparer quantitativement la différence entre la taille d’un réseau qui reçoit en entrée une représentation de vectoriseur basée sur la fréquence, comme le cas au niveau des caractères utilisé lors de l’entraînement, et un réseau qui utilise des embeddings BERT. Pour cet exercice, l’équation de comptage de paramètres Eq. (\ref{eq:param_count_fc}) pour une couche entièrement connectée est utilisée ; elle consiste en une matrice de poids et un terme de biais, et est déduite des informations fournies par PyTorch \cite{b19}.

\begin{equation}
params  = (infeatures\cdot outfeatures) + outfeatures
\label{eq:param_count_fc}
\end{equation}

En appliquant cette équation à un réseau de neurones avec la structure $\texttt{INPUT\_DIM} \rightarrow 1024 \rightarrow 512 \rightarrow 256 \rightarrow 3$, et en utilisant Eq.~(\ref{eq:param_count_mlp_char}), un total de 10{,}897{,}923 paramètres est obtenu pour le MLP, alors qu’avec la même structure mais avec une couche d’entrée adaptée à un embedding BERT, comme montré dans Eq.~(\ref{eq:param_count_mlp_bert}), 1{,}444{,}355 paramètres sont obtenus, une réduction de $\approx 86.75\%$. Cette réduction est hautement pertinente lorsqu’on vise la généralisation et la stabilité de l’entraînement, car elle diminue le risque de surapprentissage : la relation entre les données d’entraînement et les paramètres rend moins probable la mémorisation du bruit ou de dynamiques parasites. De plus, on peut obtenir un modèle moins sensible aux stratégies de régularisation et, en se concentrant uniquement sur l’entraînement du MLP, le coût computationnel de l’entraînement est réduit, un effet qui devient évident dans l’entraînement réalisé pendant la phase de test détaillée en Q2 et Q3.

\begin{equation}
\begin{gathered}
P_{1}\,(10000 \rightarrow 1024) = 10000 \cdot 1024 + 1024 = 10{,}241{,}024, \\
P_{2}\,(1024 \rightarrow 512)  = 1024 \cdot 512 + 512 = 524{,}800, \\
P_{3}\,(512 \rightarrow 256)   = 512 \cdot 256 + 256 = 131{,}328, \\
P_{4}\,(256 \rightarrow 3)     = 256 \cdot 3 + 3 = 771, \\
P_{\text{total}}               = P_{1}+P_{2}+P_{3}+P_{4} = 10{,}897{,}923.
\end{gathered}
\label{eq:param_count_mlp_char}
\end{equation}

\begin{equation}
\begin{gathered}
P_{1}\,(768 \rightarrow 1024)  = 768 \cdot 1024 + 1024 = 787{,}456, \\
P_{2}\,(1024 \rightarrow 512)  = 1024 \cdot 512 + 512 = 524{,}800, \\
P_{3}\,(512 \rightarrow 256)   = 512 \cdot 256 + 256 = 131{,}328, \\
P_{4}\,(256 \rightarrow 3)     = 256 \cdot 3 + 3 = 771, \\
P_{\text{total}}               = P_{1}+P_{2}+P_{3}+P_{4} = 1{,}444{,}355.
\end{gathered}
\label{eq:param_count_mlp_bert}
\end{equation}

La sortie des modèles de vectorisation basés sur la fréquence est généralement une matrice creuse, ce qui implique que l’information est effectivement concentrée sur seulement quelques indices actifs. Cela est directement lié non seulement à la parcimonie déjà mentionnée, mais aussi à la présence implicite de valeurs nulles dans la matrice CSR : même si elles ne sont pas explicitement stockées en mémoire, elles exposent le modèle à une sensibilité considérable à la largeur du réseau et aux méthodes de régularisation, nécessitant un contrôle strict pour éviter le surentraînement. Cela complique clairement non seulement l’obtention de performances optimales, mais aussi la définition de la structure du réseau, puisque de subtiles variations de la largeur des couches peuvent avoir un effet substantiel sur le risque de surapprentissage. En revanche, lorsqu’on utilise des embeddings BERT, la matrice est dense et de faible dimension. Cela réduit typiquement la taille du classifieur en aval nécessaire, et rend donc la sélection d’une structure de MLP et le choix des paramètres de régularisation plus faciles et moins critiques. La simplicité dans la sélection de la structure du réseau de neurones augmente encore lorsqu’on considère le troisième facteur clé proposé pour différencier les avantages des embeddings par rapport aux méthodes classiques, à savoir la sensibilité de chaque méthode au contexte.

La sensibilité au contexte est peut-être la différence la plus perceptible et la plus marquée entre les deux méthodes, et elle favorise la mise en œuvre de pipelines avec des embeddings BERT plutôt qu’avec des vectoriseurs basés sur des comptages de fréquence. Cela s’explique par le fait que BERT est solidement pré-entraîné sur des corpus linguistiques beaucoup plus vastes et, en outre, il est spécifiquement conçu pour intégrer le contexte dans la définition vectorielle des mots d’un document, grâce à la structure et au fonctionnement détaillés dans la Section Q4. En revanche, les stratégies classiques de vectorisation ne peuvent capturer qu’une information contextuelle superficielle obtenue par des comptages de fréquence ou, au mieux, par l’analyse dérivée de cet espace de fréquence, ce qui limite leur représentation d’aspects sémantiques fondamentaux pouvant être décisifs lors de la classification d’un court message dans une catégorie de sentiment/polarité. En outre, le fait qu’un embedding BERT soit si robuste, condensé et riche en information permet l’exploration de modèles plus simples pour la classification de messages. Autrement dit, dans BERT la complexité est principalement concentrée dans le pré-entraînement du modèle utilisé pour obtenir les embeddings, et le MLP est isolé de cette complexité et n’a besoin que de mapper et de classifier des vecteurs hautement informatifs. En revanche, lorsque des méthodes de vectorisation traditionnelles sont utilisées, une partie de la complexité couverte par le pré-entraînement de BERT doit être prise en charge par le réseau de neurones lui-même, ce qui rend possible, dans le cas BERT, l’utilisation de réseaux avec moins de couches, voire d’une seule tête de classification linéaire, tout en obtenant des résultats comparables à des modèles MLP plus complexes entraînés avec des techniques de vectorisation traditionnelles.

Bien que les bénéfices de l’utilisation de BERT comme extracteur d’embeddings deviennent évidents, il est essentiel de souligner qu’utiliser BERT comme extracteur d’embeddings figé n’est pas la même chose que l’utiliser adapté à la tâche, c’est-à-dire appliquer un fine-tuning qui permet aux représentations internes de s’aligner avec l’objectif de classification et avec la distribution des données d’entraînement. Cela est d’une importance critique, par exemple, dans l’analyse ou les tâches de classification de textes courts comme celle considérée ici, où l’adoption d’une stratégie de fine-tuning telle que LoRA, décrite en Q6, permet d’obtenir de meilleures performances de classifieur que l’utilisation de BERT uniquement pour une extraction d’embeddings figée. C’est grâce à ces stratégies de fine-tuning que le principal inconvénient de BERT en tant qu’extracteur d’embeddings figé peut être atténué par rapport aux vecteurs classiques, à savoir que sa représentation est externe au domaine des données d’entraînement. En d’autres termes, elle est sensible au contexte par la propre définition de BERT, mais pas à des paramètres tels que la fréquence de présence des tokens dans l’ensemble d’entraînement et d’autres propriétés qui apparaissent parce que l’entraînement du transformeur est déjà fixé et, dans l’extraction d’embeddings figée, aucun apprentissage n’a lieu.

Il est finalement conclu que BERT est plus attractif que les méthodes classiques principalement en raison de sa capacité à représenter la sémantique et le contexte, ce que les méthodes classiques ne possèdent pas. Cela est donc associé à une entrée plus dense qui requiert moins de complexité dans le MLP de classification. De plus, la possibilité de mettre en œuvre des stratégies de fine-tuning qui contrebalancent l’effet agnostique au domaine du pré-entraînement de l’extracteur d’embeddings, et qui permettent au domaine du jeu de données d’entraînement de perméer la représentation d’embedding du document, peut améliorer substantiellement les performances du modèle.
